{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a9f8b7c6_intro",
      "metadata": {},
      "source": [
        "# COSC 4337 - Homework 3: RNNs for Time Series Forecasting (Netflix Stocks) ðŸ“ˆ\n",
        "\n",
        "**Objective:** The goal of this assignment is to build, train, and evaluate Recurrent Neural Networks (RNNs), including GRU and LSTM variants, for predicting Netflix stock prices.\n",
        "\n",
        "***Note on Frameworks***\n",
        "\n",
        "This notebook uses `Keras` with a `TensorFlow` backend. If you are more comfortable with `PyTorch`, you are welcome to complete the assignment using it. The core concepts are directly transferable. Feel free to change code as you see fit as long as it follows the tasks.\n",
        "\n",
        "## Problem Statement\n",
        "\n",
        "You are tasked with predicting the **closing price** of Netflix (NFLX) stock for the next day based on historical daily data.\n",
        "\n",
        "**Dataset:**\n",
        "We will use a dataset containing historical daily stock prices for Netflix (NFLX), including Open, High, Low, Close, Adj Close, and Volume. The specific file used (`NFLX.csv`) covers 2018-2022.\n",
        "\n",
        "**Dataset Features (Input):**\n",
        "1.  A sequence of a *look_back* number of past closing prices (or other features in later exercises).\n",
        "\n",
        "**Dataset Target (Output):**\n",
        "* The closing price for the *next* trading day.\n",
        "\n",
        "Link to original dataset source concept: https://www.kaggle.com/datasets/jainilcoder/netflix-stock-price-prediction\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8a0c1b3_imports",
      "metadata": {},
      "source": [
        "## Setup: Import Libraries and Set Seed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8a0c1b3_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Import Libraries ===\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import random\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN, GRU, LSTM, Dropout\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# === Set Seed ===\n",
        "seed = 4337\n",
        "np.random.seed(seed)\n",
        "random.set_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4f5a6b7_dataload_head",
      "metadata": {},
      "source": [
        "## Data Loading and Initial Exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7g8h9i0_load",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 1. Load Data ===\n",
        "\n",
        "data_path = \n",
        "\n",
        "# YOUR CODE HERE: Load the dataset from the url into a pandas DataFrame named 'df'.\n",
        "# Make sure to parse the 'Date' column as datetime objects using the 'parse_dates' argument.\n",
        "df = # ... YOUR CODE ...\n",
        "\n",
        "# Display the first few rows\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "# ... YOUR CODE ...\n",
        "\n",
        "# Display basic info about the DataFrame\n",
        "print(\"\\nDataFrame Info:\")\n",
        "# ... YOUR CODE ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4f5a6b7_select_plot",
      "metadata": {},
      "source": [
        "## Select Target Variable and Visualize\n",
        "\n",
        "We are selecting the 'Close' price as our target variable and plotting it over time.\n",
        "We need to isolate the variable we want to predict. Plotting helps visualize the trends, seasonality, and potential challenges in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7g8h9i0_select_plot_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 2a. Select Target Variable ===\n",
        "# YOUR CODE HERE: Extract the 'Close' column as a numpy array, ensure it's float32, and reshape it to be a 2D array (samples, 1 feature).\n",
        "# Store this in a variable called 'close_prices'.\n",
        "close_prices = \n",
        "\n",
        "print(f\"Shape of close_prices: {close_prices.shape}\")\n",
        "\n",
        "# === 2b. Visualize the Close Price ===\n",
        "plt.figure(figsize=(14, 7))\n",
        "# YOUR CODE HERE: Plot the 'Date' column from the DataFrame 'df' on the x-axis\n",
        "# and the 'close_prices' array on the y-axis.\n",
        "# Add a title 'Netflix Close Price History', xlabel 'Date', and ylabel 'Close Price USD'.\n",
        "\n",
        "plt.plot(# ... YOUR CODE ... )\n",
        "plt.title(# ... YOUR CODE ... )\n",
        "plt.xlabel(# ... YOUR CODE ... )\n",
        "plt.ylabel(# ... YOUR CODE ... )\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4f5a6b7_scale",
      "metadata": {},
      "source": [
        "## Scale the Data\n",
        "\n",
        "Neural networks generally perform better with normalized input data. Scaling prevents features with large values from disproportionately influencing the model's learning process and helps gradients flow more effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7g8h9i0_scale_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 3. Scale the Data ===\n",
        "# YOUR CODE HERE: Initialize a MinMaxScaler with feature_range=(0, 1).\n",
        "scaler = # ... YOUR CODE ...\n",
        "\n",
        "# YOUR CODE HERE: Apply the scaler to 'close_prices' using fit_transform and store the result in 'dataset_scaled'.\n",
        "dataset_scaled = # ... YOUR CODE ...\n",
        "\n",
        "print(\"Shape of scaled dataset:\", dataset_scaled.shape)\n",
        "print(\"First 5 scaled values:\\n\", dataset_scaled[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4f5a6b7_sequence_def",
      "metadata": {},
      "source": [
        "## Define Sequence Creation Function\n",
        "\n",
        "We are creating a reusable function to transform the time series data into input sequences (X) and target outputs (y).\n",
        "Define a function `create_sequences` that takes the scaled data and `look_back` period as input. It iterates through the data, creating sequences of length `look_back` and pairing them with the immediately following data point as the target. RNNs require input data structured as sequences. This function automates the process of generating these sequence-target pairs, which is fundamental for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7g8h9i0_sequence_def_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 4. Define Sequence Creation Function ===\n",
        "def create_sequences(data, look_back=1):\n",
        "    \"\"\"Converts univariate time series data into sequences for RNNs.\"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - look_back):\n",
        "        sequence = data[i:(i + look_back), 0]\n",
        "        target = data[i + look_back, 0]\n",
        "        X.append(sequence)\n",
        "        y.append(target)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Test the function (optional)\n",
        "test_data = np.arange(10).reshape(-1, 1)\n",
        "testX, testY = create_sequences(test_data, 3)\n",
        "print(\"Test Data:\\n\", test_data.flatten())\n",
        "print(\"Test X (look_back=3):\\n\", testX)\n",
        "print(\"Test Y (look_back=3):\\n\", testY)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4f5a6b7_sequence_apply",
      "metadata": {},
      "source": [
        "## Create Sequences and Reshape\n",
        "\n",
        "We are applying the `create_sequences` function and reshaping the resulting input data `X`.\n",
        "Call the function with the scaled data and the desired `look_back`. Then use `np.reshape` on `X`.\n",
        "We are doing this to generate the actual training/testing sequences based on our chosen lookback period (e.g., 60 days). The reshaping step is mandatory to get the 3D format `[samples, timesteps, features]` required by Keras RNN layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7g8h9i0_sequence_apply_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 5a. Create Sequences ===\n",
        "# We'll use the past 60 days (timesteps) to predict the next day.\n",
        "look_back = 60\n",
        "\n",
        "# YOUR CODE HERE: Call the 'create_sequences' function with 'dataset_scaled' and 'look_back'.\n",
        "# Store the results in variables 'X' and 'y'.\n",
        "X, y = # ... YOUR CODE ...\n",
        "\n",
        "print(f\"Created {X.shape[0]} sequences.\")\n",
        "print(\"Shape of X before reshape:\", X.shape)\n",
        "print(\"Shape of y:\", y.shape)\n",
        "\n",
        "# === 5b. Reshape Input Data for RNN ===\n",
        "# Reshape X to the required 3D format: [samples, time steps, features]\n",
        "# YOUR CODE HERE: Reshape 'X' using np.reshape. The new shape should have:\n",
        "# - X.shape[0] as the number of samples\n",
        "# - X.shape[1] (which is look_back) as the number of timesteps\n",
        "# - 1 as the number of features (since we are only using 'Close' price)\n",
        "X = # ... YOUR CODE ...\n",
        "\n",
        "print(\"Shape of X after reshape:\", X.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4f5a6b7_split",
      "metadata": {},
      "source": [
        "## Split Data into Training and Testing Sets\n",
        "\n",
        "We are dividing the sequential data into a set for training the model and a separate set for evaluating its performance.\n",
        "Calculate a split point (e.g., 80% mark) and slicing the `X` and `y` arrays. \n",
        "For time series forecasting, the model must learn from the past to predict the future. We train on the earlier portion of the data and test on the later portion to mimic this real-world scenario.\n",
        "\n",
        "**Crucial to notice that we do not shuffle the data.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7g8h9i0_split_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 6. Train/Test Split (Sequential) ===\n",
        "# Use 80% of the data for training, 20% for testing.\n",
        "train_size_percentage = 0.80\n",
        "\n",
        "# YOUR CODE HERE: Calculate the integer index 'train_size' representing the end of the training data.\n",
        "train_size = # ... YOUR CODE ...\n",
        "\n",
        "# YOUR CODE HERE: Slice the 'X' and 'y' arrays to create:\n",
        "# - X_train, y_train (from the beginning up to train_size)\n",
        "# - X_test, y_test (from train_size to the end)\n",
        "X_train, X_test = # ... YOUR CODE ...\n",
        "y_train, y_test = # ... YOUR CODE ...\n",
        "\n",
        "# --- Verification --- \n",
        "print(\"--- Split Shapes --- \")\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g1h2i3j4_build_head",
      "metadata": {},
      "source": [
        "## Exercise 1: Build and Compare RNN Models\n",
        "\n",
        "For this task define, compile, and train three different types of recurrent neural networks: SimpleRNN, GRU, and LSTM.\n",
        "\n",
        "We will compare the performance of these architectures. Stock prices have **long-term dependencies** (e.g., the price 60 days ago can still influence today's trend). \n",
        "* **SimpleRNNs** struggle with this due to the **vanishing gradient problem**.\n",
        "* **LSTMs (Long Short-Term Memory)** use gates (forget, input, output) and a cell state to selectively remember or forget information over long sequences, mitigating the vanishing gradient issue.\n",
        "* **GRUs (Gated Recurrent Units)** are a simplified version of LSTMs with fewer gates (update, reset), often achieving similar performance with less computation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g1h2i3j4_build_defs",
      "metadata": {},
      "source": [
        "### 1a. Define Model Architectures\n",
        "Specify the layers for each model.\n",
        "Use `Sequential()` and add the appropriate RNN layer (`SimpleRNN`, `GRU`, `LSTM`) with 32 units, followed by a `Dense` output layer with 1 unit.\n",
        "This sets up the structure of each network. 32 units is an arbitrary starting point for hidden layer complexity. The `Dense(1)` layer is required for our regression task (predicting one continuous value)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k5l6m7n8_defs",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Define Input Shape ===\n",
        "input_shape = (look_back, 1)\n",
        "\n",
        "# --- Define Model 1 (SimpleRNN) ---\n",
        "model_rnn = Sequential(name=\"SimpleRNN_Model\")\n",
        "# YOUR CODE HERE: Add a SimpleRNN layer with 32 units. Specify the input_shape.\n",
        "model_rnn.add(# ... YOUR CODE ...\n",
        "# YOUR CODE HERE: Add a Dense output layer with 1 unit.\n",
        "model_rnn.add(# ... YOUR CODE ...\n",
        "\n",
        "# --- Define Model 2 (GRU) ---\n",
        "model_gru = Sequential(name=\"GRU_Model\")\n",
        "# YOUR CODE HERE: Add a GRU layer with 32 units. Specify the input_shape.\n",
        "model_gru.add(# ... YOUR CODE ...\n",
        "# YOUR CODE HERE: Add a Dense output layer with 1 unit.\n",
        "model_gru.add(# ... YOUR CODE ...\n",
        "\n",
        "# --- Define Model 3 (LSTM) ---\n",
        "model_lstm = Sequential(name=\"LSTM_Model\")\n",
        "# YOUR CODE HERE: Add an LSTM layer with 32 units. Specify the input_shape.\n",
        "model_lstm.add(# ... YOUR CODE ...\n",
        "# YOUR CODE HERE: Add a Dense output layer with 1 unit.\n",
        "model_lstm.add(# ... YOUR CODE ...\n",
        "\n",
        "print(\"Models defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g1h2i3j4_build_compile",
      "metadata": {},
      "source": [
        "### 1b. Compile Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k5l6m7n8_compile",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Compile all models ---\n",
        "print(\"Compiling models...\")\n",
        "# YOUR CODE HERE: Compile model_rnn with 'adam' optimizer and 'mean_squared_error' loss.\n",
        "model_rnn.compile(# ... YOUR CODE ...\n",
        "\n",
        "# YOUR CODE HERE: Compile model_gru similarly.\n",
        "model_gru.compile(# ... YOUR CODE ...\n",
        "\n",
        "# YOUR CODE HERE: Compile model_lstm similarly.\n",
        "model_lstm.compile(# ... YOUR CODE ...\n",
        "\n",
        "# --- Print Model Summary (Example) ---\n",
        "print(\"\\n--- Model 3 (LSTM) Summary ---\")\n",
        "model_lstm.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o9p0q1r2_train_head",
      "metadata": {},
      "source": [
        "### 1c. Train the Models\n",
        "\n",
        "\n",
        " We use `validation_split=0.1` to monitor performance on unseen data during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "train_rnn_cell",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "epochs = 50 # Number of passes through the entire training dataset.\n",
        "batch_size = 32 # Number of samples processed before the model's weights are updated.\n",
        "\n",
        "print(f\"Training Model 1 (SimpleRNN) for {epochs} epochs...\")\n",
        "# YOUR CODE HERE: Train 'model_rnn' using .fit().\n",
        "# Pass X_train, y_train, epochs, batch_size, validation_split=0.1, and verbose=1.\n",
        "# Store the output history in 'history_rnn'.\n",
        "history_rnn = # ... YOUR CODE ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "train_gru_cell",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nTraining Model 2 (GRU) for {epochs} epochs...\")\n",
        "# YOUR CODE HERE: Train 'model_gru' similarly to model_rnn.\n",
        "# Store the output history in 'history_gru'.\n",
        "history_gru = # ... YOUR CODE ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "train_lstm_cell",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nTraining Model 3 (LSTM) for {epochs} epochs...\")\n",
        "# YOUR CODE HERE: Train 'model_lstm' similarly to model_rnn.\n",
        "# Store the output history in 'history_lstm'.\n",
        "history_lstm = # ... YOUR CODE ...\n",
        "\n",
        "print(\"\\nAll models trained successfully! ðŸŽ‰\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "w7x8y9z0_eval_head",
      "metadata": {},
      "source": [
        "### 1d. Evaluate Models on Test Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1b2c3d4_eval_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Evaluate Models on Test Set ===\n",
        "print(\"\\nEvaluating models on the test set...\")\n",
        "\n",
        "# YOUR CODE HERE: Evaluate 'model_rnn' on X_test and y_test. Store the result in 'mse_rnn'.\n",
        "# Set verbose=0 to avoid printing evaluation progress.\n",
        "mse_rnn = # ... YOUR CODE ...\n",
        "print(f\"Model 1 (SimpleRNN) - Test MSE: {mse_rnn:.6f}\")\n",
        "\n",
        "# YOUR CODE HERE: Evaluate 'model_gru' on X_test and y_test. Store the result in 'mse_gru'.\n",
        "mse_gru = # ... YOUR CODE ...\n",
        "print(f\"Model 2 (GRU) - Test MSE: {mse_gru:.6f}\")\n",
        "\n",
        "# YOUR CODE HERE: Evaluate 'model_lstm' on X_test and y_test. Store the result in 'mse_lstm'.\n",
        "mse_lstm = # ... YOUR CODE ...\n",
        "print(f\"Model 3 (LSTM) - Test MSE: {mse_lstm:.6f}\")\n",
        "\n",
        "# Calculate Root Mean Squared Error (RMSE) for better interpretability (optional)\n",
        "rmse_rnn = np.sqrt(mse_rnn)\n",
        "rmse_gru = np.sqrt(mse_gru)\n",
        "rmse_lstm = np.sqrt(mse_lstm)\n",
        "\n",
        "print(f\"\\nModel 1 (SimpleRNN) - Test RMSE: {rmse_rnn:.6f} (scaled units)\")\n",
        "print(f\"Model 2 (GRU) - Test RMSE: {rmse_gru:.6f} (scaled units)\")\n",
        "print(f\"Model 3 (LSTM) - Test RMSE: {rmse_lstm:.6f} (scaled units)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5f6g7h8_eval_tune_head",
      "metadata": {},
      "source": [
        "## Exercise 2: More Evaluation & Tuning Experiments\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5f6g7h8_loss_curves_head",
      "metadata": {},
      "source": [
        "### 2a. Plotting Learning Curves\n",
        "We will visualize how training and validation loss changed during training for each model.\n",
        "Define a function to plot `loss` and `val_loss` from a model's history object. Call this function for each of the three trained models.\n",
        "This helps diagnose **overfitting**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i9j0k1l2_plot_loss_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_loss_curves(history, model_name):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    # YOUR CODE HERE: Plot the training loss (history.history['loss'])\n",
        "    plt.plot(# ... YOUR CODE ... , label='Train Loss')\n",
        "    # YOUR CODE HERE: Plot the validation loss (history.history['val_loss'])\n",
        "    plt.plot(# ... YOUR CODE ... , label='Validation Loss')\n",
        "    plt.title(f'{model_name}: Training & Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Mean Squared Error (Loss)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\nPlotting Loss Curves for Each Model...\")\n",
        "# YOUR CODE HERE: Call plot_loss_curves for history_rnn with model name 'SimpleRNN'.\n",
        "# ... YOUR CODE ...\n",
        "\n",
        "# YOUR CODE HERE: Call plot_loss_curves for history_gru with model name 'GRU'.\n",
        "# ... YOUR CODE ...\n",
        "\n",
        "# YOUR CODE HERE: Call plot_loss_curves for history_lstm with model name 'LSTM'.\n",
        "# ... YOUR CODE ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5f6g7h8_pred_plot_head",
      "metadata": {},
      "source": [
        "### 2b. Plotting Predictions vs. Actual\n",
        "We will compare the models predictions on the test set against the actual stock prices visually. We can see if the models capture the overall trend, magnitude, and turning points of the actual stock price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i9j0k1l2_plot_preds_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nGenerating predictions for final plot...\")\n",
        "# === Make Predictions ===\n",
        "# YOUR CODE HERE: Predict on X_test using model_rnn, store in test_predict_rnn\n",
        "test_predict_rnn = # ... YOUR CODE ...\n",
        "# YOUR CODE HERE: Predict on X_test using model_gru, store in test_predict_gru\n",
        "test_predict_gru = # ... YOUR CODE ...\n",
        "# YOUR CODE HERE: Predict on X_test using model_lstm, store in test_predict_lstm\n",
        "test_predict_lstm = # ... YOUR CODE ...\n",
        "\n",
        "# === Inverse Transform Predictions ===\n",
        "# What: Convert scaled predictions (0-1 range) back to original dollar values.\n",
        "# How: Use scaler.inverse_transform().\n",
        "# Why: To compare predictions directly with the actual stock prices.\n",
        "# YOUR CODE HERE: Inverse transform test_predict_rnn\n",
        "test_predict_rnn = # ... YOUR CODE ...\n",
        "# YOUR CODE HERE: Inverse transform test_predict_gru\n",
        "test_predict_gru = # ... YOUR CODE ...\n",
        "# YOUR CODE HERE: Inverse transform test_predict_lstm\n",
        "test_predict_lstm = # ... YOUR CODE ...\n",
        "\n",
        "# === Inverse Transform Actual Test Data ===\n",
        "# YOUR CODE HERE: Inverse transform y_test. Reshape y_test to 2D first using .reshape(-1, 1).\n",
        "y_test_actual = # ... YOUR CODE ...\n",
        "\n",
        "# === Plotting === #\n",
        "plt.figure(figsize=(15, 7))\n",
        "\n",
        "# Get the dates corresponding to the test set\n",
        "test_dates = df['Date'].iloc[train_size+look_back:]\n",
        "\n",
        "# YOUR CODE HERE: Plot the actual test data (test_dates vs y_test_actual). Label it 'Actual Netflix Close Price'.\n",
        "plt.plot(# ... YOUR CODE ... )\n",
        "\n",
        "# YOUR CODE HERE: Plot the RNN test predictions (test_dates vs test_predict_rnn). Label it 'RNN Test Prediction'. Use linestyle='--'.\n",
        "plt.plot(# ... YOUR CODE ... )\n",
        "\n",
        "# YOUR CODE HERE: Plot the GRU test predictions (test_dates vs test_predict_gru). Label it 'GRU Test Prediction'. Use linestyle='--'.\n",
        "plt.plot(# ... YOUR CODE ... )\n",
        "\n",
        "# YOUR CODE HERE: Plot the LSTM test predictions (test_dates vs test_predict_lstm). Label it 'LSTM Test Prediction'. Use linestyle='--'.\n",
        "plt.plot(# ... YOUR CODE ... )\n",
        "\n",
        "plt.title('Netflix Stock Price Prediction Comparison (Test Set)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price (USD)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m3n4o5p6_stacked_head",
      "metadata": {},
      "source": [
        "### 2c. Experiment: Stacked LSTM\n",
        "We will build a deeper LSTM network with two layers. By Adding a second `LSTM(32)` layer. The *first* LSTM layer needs `return_sequences=True`.\n",
        "Stacking allows the network to potentially learn more complex, hierarchical temporal patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q7r8s9t0_stacked_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- Experiment: Stacked LSTM ---\")\n",
        "model_stacked_lstm = Sequential(name=\"Stacked_LSTM_Model\")\n",
        "\n",
        "# YOUR CODE HERE: Add the first LSTM layer with 32 units.\n",
        "# Remember to set return_sequences=True and specify input_shape.\n",
        "model_stacked_lstm.add(# ... YOUR CODE ...\n",
        "\n",
        "# YOUR CODE HERE: Add the second LSTM layer with 32 units.\n",
        "# return_sequences defaults to False, which is correct here.\n",
        "model_stacked_lstm.add(# ... YOUR CODE ...\n",
        "\n",
        "# YOUR CODE HERE: Add the Dense output layer with 1 unit.\n",
        "model_stacked_lstm.add(# ... YOUR CODE ...\n",
        "\n",
        "# YOUR CODE HERE: Compile the model ('adam', 'mean_squared_error').\n",
        "model_stacked_lstm.compile(# ... YOUR CODE ...\n",
        "\n",
        "model_stacked_lstm.summary()\n",
        "\n",
        "# --- Train and Evaluate Stacked LSTM ---\n",
        "print(f\"\\nTraining Stacked LSTM for {epochs} epochs...\")\n",
        "# YOUR CODE HERE: Train the 'model_stacked_lstm' using .fit().\n",
        "# Use X_train, y_train, epochs, batch_size, validation_split=0.1, verbose=0.\n",
        "# Store the history in 'history_stacked'.\n",
        "history_stacked = # ... YOUR CODE ...\n",
        "\n",
        "# YOUR CODE HERE: Evaluate the trained 'model_stacked_lstm' on the test set.\n",
        "# Store the result in 'mse_stacked'.\n",
        "mse_stacked = # ... YOUR CODE ...\n",
        "print(f\"Stacked LSTM - Test MSE: {mse_stacked:.6f}\")\n",
        "\n",
        "# YOUR CODE HERE: Plot the loss curves for the stacked model using plot_loss_curves().\n",
        "# ... YOUR CODE ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u1v2w3x4_dropout_head",
      "metadata": {},
      "source": [
        "### 2d. Experiment: LSTM with Dropout\n",
        "We are adding Dropout regularization to the simple LSTM model by Inserting a `Dropout(0.2)` layer *after* the LSTM layer and *before* the final Dense layer.\n",
        "Dropout randomly deactivates a fraction of neurons during training, forcing the network to learn more robust representations and reducing the risk of overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y5z6a7b8_dropout_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- Experiment: LSTM with Dropout ---\")\n",
        "model_dropout_lstm = Sequential(name=\"Dropout_LSTM_Model\")\n",
        "\n",
        "# YOUR CODE HERE: Add an LSTM layer with 32 units, specifying input_shape.\n",
        "model_dropout_lstm.add(# ... YOUR CODE ...\n",
        "\n",
        "# YOUR CODE HERE: Add a Dropout layer with a rate of 0.2 (20% dropout).\n",
        "model_dropout_lstm.add(# ... YOUR CODE ...\n",
        "\n",
        "# YOUR CODE HERE: Add the Dense output layer with 1 unit.\n",
        "model_dropout_lstm.add(# ... YOUR CODE ...\n",
        "\n",
        "# YOUR CODE HERE: Compile the model ('adam', 'mean_squared_error').\n",
        "model_dropout_lstm.compile(# ... YOUR CODE ...\n",
        "\n",
        "model_dropout_lstm.summary()\n",
        "\n",
        "# --- Train and Evaluate Dropout LSTM ---\n",
        "print(f\"\\nTraining LSTM with Dropout for {epochs} epochs...\")\n",
        "# YOUR CODE HERE: Train 'model_dropout_lstm' using .fit().\n",
        "# Store the history in 'history_dropout'.\n",
        "history_dropout = # ... YOUR CODE ...\n",
        "\n",
        "# YOUR CODE HERE: Evaluate the trained 'model_dropout_lstm' on the test set.\n",
        "# Store the result in 'mse_dropout'.\n",
        "mse_dropout = # ... YOUR CODE ...\n",
        "print(f\"LSTM with Dropout - Test MSE: {mse_dropout:.6f}\")\n",
        "\n",
        "# YOUR CODE HERE: Plot the loss curves for the dropout model.\n",
        "# ... YOUR CODE ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "new_ex_lookback_head",
      "metadata": {},
      "source": [
        "### 2e. Experiment: Different Lookback Period\n",
        "Re-train the best performing model (likely LSTM or GRU) using a different `look_back` period.\n",
        "Repeat the sequence creation, reshaping, and train/test split steps with `look_back = 30`. Then define, compile, train, and evaluate the chosen model architecture again.\n",
        "The choice of lookback period is a hyperparameter. A shorter period (30 days) might focus more on recent trends, while a longer one (60 days) captures more historical context. This experiment helps understand how sensitive the model is to this choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "new_ex_lookback_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- Experiment: Shorter Lookback (30 days) with LSTM ---\")\n",
        "look_back_short = 30\n",
        "\n",
        "# YOUR CODE HERE: Re-create sequences using 'dataset_scaled' and 'look_back_short'. Store in X_short, y_short.\n",
        "X_short, y_short = # ... YOUR CODE ...\n",
        "\n",
        "# YOUR CODE HERE: Reshape X_short to the 3D format [samples, timesteps, features].\n",
        "X_short = # ... YOUR CODE ...\n",
        "\n",
        "# YOUR CODE HERE: Perform train/test split on X_short, y_short (80/20 split).\n",
        "# Use the same train_size_percentage. Store results in X_train_short, X_test_short, y_train_short, y_test_short.\n",
        "train_size_short = # ... YOUR CODE ...\n",
        "X_train_short, X_test_short = # ... YOUR CODE ...\n",
        "y_train_short, y_test_short = # ... YOUR CODE ...\n",
        "\n",
        "print(\"Short Lookback Shapes:\")\n",
        "print(\"X_train_short:\", X_train_short.shape, \"y_train_short:\", y_train_short.shape)\n",
        "print(\"X_test_short:\", X_test_short.shape, \"y_test_short:\", y_test_short.shape)\n",
        "\n",
        "# --- Build, Compile, Train, Evaluate LSTM with Short Lookback ---\n",
        "model_lstm_short = Sequential(name=\"LSTM_ShortLookback_Model\")\n",
        "# YOUR CODE HERE: Add an LSTM layer with 32 units. The input_shape will use look_back_short.\n",
        "model_lstm_short.add(# ... YOUR CODE ...\n",
        "# YOUR CODE HERE: Add the Dense output layer.\n",
        "model_lstm_short.add(# ... YOUR CODE ...\n",
        "\n",
        "# YOUR CODE HERE: Compile the model.\n",
        "model_lstm_short.compile(# ... YOUR CODE ...\n",
        "\n",
        "print(f\"\\nTraining LSTM with {look_back_short}-day Lookback...\")\n",
        "# YOUR CODE HERE: Train 'model_lstm_short' using the _short datasets.\n",
        "# Store history in 'history_lstm_short'. Use verbose=0.\n",
        "history_lstm_short = # ... YOUR CODE ...\n",
        "\n",
        "# YOUR CODE HERE: Evaluate 'model_lstm_short' on the _short test set.\n",
        "# Store mse in 'mse_lstm_short'.\n",
        "mse_lstm_short = # ... YOUR CODE ...\n",
        "print(f\"LSTM with {look_back_short}-day Lookback - Test MSE: {mse_lstm_short:.6f}\")\n",
        "\n",
        "# YOUR CODE HERE: Plot the loss curves for this short lookback model.\n",
        "# ... YOUR CODE ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9d0e1f2_analysis_head",
      "metadata": {},
      "source": [
        "## Exercise 3: Analysis and Questions\n",
        "\n",
        "**Instructions:** Answer the following questions based on the results you obtained by running the code above. Refer back to the MSE values, loss curves, and prediction plots."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g3h4i5j6_q1",
      "metadata": {},
      "source": [
        "**1. Initial Model Comparison (Ex 1):** \n",
        "    * Compare the **Test MSE** results from the SimpleRNN, GRU, and LSTM models (from section 1d).\n",
        "    * Which model performed best (lowest Test MSE)? Which performed worst?\n",
        "    * Does this outcome align with the theoretical advantages of LSTMs/GRUs (handling long-term dependencies better than SimpleRNNs)? Explain briefly."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "k7l8m9n0_a1",
      "metadata": {},
      "source": [
        "*(Your Answer Here)*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o1p2q3r4_q2",
      "metadata": {},
      "source": [
        "**2. Prediction Plot Analysis (Ex 2b):** \n",
        "    * Looking at the plot comparing actual vs. predicted prices on the test set, which model's predictions (dashed lines) visually seemed to follow the actual price trend most closely?\n",
        "    * Does the model that *looks* best on the plot necessarily have the lowest Test MSE you recorded in Ex 1d? Discuss any similarities or differences between your visual assessment and the MSE scores."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s5t6u7v8_a2",
      "metadata": {},
      "source": [
        "*(Your Answer Here)*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "w9x0y1z2_q3",
      "metadata": {},
      "source": [
        "**3. Task Type & Configuration:**\n",
        "    * Why did we use `mean_squared_error` as the loss function for predicting stock prices, instead of a classification loss like `categorical_crossentropy`?\n",
        "    * Why does the final `Dense` layer in all models have only 1 unit and use a `linear` (default) activation function?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3b4c5d6_a3",
      "metadata": {},
      "source": [
        "*(Your Answer Here)*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7f8g9h0_q4",
      "metadata": {},
      "source": [
        "**4. Input Shape Explanation:** For the input shape `[samples, timesteps, features]` used in this assignment:\n",
        "    * What does the `samples` dimension represent (how many sequences did we create)?\n",
        "    * What does the `timesteps` dimension represent (what value did we set for `look_back` initially)?\n",
        "    * What does the `features` dimension represent (how many features did we use, and what was it)?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i1j2k3l4_a4",
      "metadata": {},
      "source": [
        "*(Your Answer Here)*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m5n6o7p8_q5",
      "metadata": {},
      "source": [
        "**5. Stacked LSTM (`return_sequences=True`) (Ex 2c):**\n",
        "    * Explain *why* setting `return_sequences=True` on the *first* LSTM layer was necessary for the stacked model.\n",
        "    * What is the shape of the output from an LSTM layer when `return_sequences=True` vs. `return_sequences=False` (the default)?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q9r0s1t2_a5",
      "metadata": {},
      "source": [
        "*(Your Answer Here)*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u3v4w5x6_q6",
      "metadata": {},
      "source": [
        "**6. Tuning Results Analysis (Ex 2c, 2d):** \n",
        "    * Compare the **Test MSE** of the simple LSTM (Ex 1d), the Stacked LSTM (Ex 2c), and the LSTM with Dropout (Ex 2d).\n",
        "    * Did stacking LSTM layers improve performance over the single LSTM layer in this case? Suggest a reason why it might have helped or hurt.\n",
        "    * Did adding Dropout improve performance over the single LSTM layer? Look at the loss curves for the Dropout model and the simple LSTM - does Dropout appear to reduce overfitting (i.e., is the gap between train and validation loss smaller)? Explain."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y7z8a9b0_a6",
      "metadata": {},
      "source": [
        "*(Your Answer Here)*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "new_ex_lookback_q7",
      "metadata": {},
      "source": [
        "**7. Lookback Period Analysis (Ex 2e):**\n",
        "    * Compare the **Test MSE** of the LSTM model with the original lookback (60 days, from Ex 1d) and the LSTM model with the shorter lookback (30 days, from Ex 2e).\n",
        "    * Which lookback period resulted in better performance on the test set?\n",
        "    * Why might a shorter or longer lookback period be better for predicting stock prices?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "new_ex_lookback_a7",
      "metadata": {},
      "source": [
        "*(Your Answer Here)*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1d2e3f4_deliverables",
      "metadata": {},
      "source": [
        "## 4. Deliverables\n",
        "\n",
        "Please submit:\n",
        "1.  This completed Jupyter Notebook file (`.ipynb`) with all code cells executed and outputs (including plots and MSE values) clearly visible. **(25% penalty if submitted notebok is not executed)** \n",
        "2.  An exported HTML version or PDF version of this notebook (`File -> Download as -> HTML`). **(25% penalty if HTML or PDF is not submitted)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "487c933e",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
